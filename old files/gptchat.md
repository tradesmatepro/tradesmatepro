Got it ‚úÖ ‚Äî if the goal is Claude can run fix loops himself without you manually stepping in, then the deployer needs two guaranteed abilities:

Full Schema Introspection (so Claude knows the exact truth of what‚Äôs in the DB ‚Äî tables, columns, indexes, constraints, triggers, functions).

Reliable Deployment of Any Layer (so Claude can re-apply enums, tables, columns, constraints, indexes, triggers, functions ‚Äî without rollback loops).

üîé Problems Today

Deployer auto-fix regex ‚Üí good for CREATE TABLE / ALTER TABLE, but breaks on PL/pgSQL ($$ ‚Ä¶ $$ blocks).

Functions layer ‚Üí one failure = whole transaction rollback.

Claude doesn‚Äôt know the live DB state ‚Üí he guesses instead of comparing against reality.

üéØ What Needs To Be Added
1. Schema Dump Mode (--dump)

Add a mode to deploy-enhanced.js that runs introspection queries and writes JSON files (so Claude can parse them).

Tables + Columns

SELECT table_name, column_name, data_type
FROM information_schema.columns
WHERE table_schema = 'public'
ORDER BY table_name, ordinal_position;


Constraints

SELECT conrelid::regclass AS table_name,
       conname AS constraint_name,
       pg_get_constraintdef(c.oid)
FROM pg_constraint c
JOIN pg_namespace n ON n.oid = c.connamespace
WHERE n.nspname = 'public';


Indexes

SELECT schemaname, relname AS table_name, indexname, indexdef
FROM pg_indexes
WHERE schemaname = 'public';


Triggers

SELECT event_object_table AS table_name,
       trigger_name,
       event_manipulation AS event,
       action_statement
FROM information_schema.triggers
WHERE trigger_schema = 'public';


Functions

SELECT proname AS function_name,
       pg_get_functiondef(p.oid) AS definition
FROM pg_proc p
JOIN pg_namespace n ON n.oid = p.pronamespace
WHERE n.nspname = 'public';


üëâ Output ‚Üí /deploy/exports/schema_dump.json.
Claude can now always ‚Äúsee truth‚Äù instead of guessing.

2. Functions-Only Mode (--functions-only)

Deploy functions individually instead of one giant transaction.

Wrap each in its own BEGIN; ‚Ä¶ COMMIT;.

Log failures without halting the rest.

Example pseudocode for deployer:

if (args.includes('--functions-only')) {
  const sql = fs.readFileSync('deploy/phase1/functions.sql', 'utf8');
  const functions = sql.split(/^--\s*FUNCTION\s*/m); // split by marker
  for (const fn of functions) {
    if (!fn.trim()) continue;
    try {
      await client.query('BEGIN');
      await client.query(fn);
      await client.query('COMMIT');
    } catch (err) {
      await client.query('ROLLBACK');
      logError(fn, err); // send to error server
    }
  }
}

3. Smarter Auto-Fix

Regex should skip inside $$ ‚Ä¶ $$ blocks so it doesn‚Äôt mangle function bodies.

Only apply auto-fix to CREATE TABLE, ALTER TABLE, etc.

‚úÖ Result

With these enhancements:

Claude can run:

node deploy-enhanced.js --dump


‚Üí Full schema snapshot into JSON.

Claude can run:

node deploy-enhanced.js --functions-only


‚Üí Re-deploy just functions in a safe loop (no rollback nukes).

Claude can run normal phases (--phase=1) for tables/columns/enums like before.

Error server keeps logging so you know exactly what failed.

üëâ Do you want me to write the actual --dump and --functions-only code additions for deploy-enhanced.js, so you can drop them in and Claude will be able to handle his own fix loops?

You said:
yes
ChatGPT said:

Perfect ‚úÖ ‚Äî let‚Äôs extend your deploy-enhanced.js so Claude can:

Dump the full schema (--dump) into /deploy/exports/schema_dump.json.

Deploy functions safely (--functions-only) one by one, without rolling back everything if one fails.

üìã Enhanced deploy-enhanced.js Additions

Drop these right into your existing file. I‚Äôll mark the new sections clearly so you can merge without breaking current deployer logic.

#!/usr/bin/env node
import fs from "fs";
import pg from "pg";
import dotenv from "dotenv";

dotenv.config();
const { Client } = pg;

const args = process.argv.slice(2);
const connString = process.env.DATABASE_URL || process.env.DB_URL;

if (!connString) {
  console.error("‚ùå Missing DATABASE_URL or DB_URL in .env");
  process.exit(1);
}

const client = new Client({ connectionString: connString, ssl: { rejectUnauthorized: false } });
await client.connect();

//
// ========== NEW FEATURE: SCHEMA DUMP MODE ==========
//
if (args.includes("--dump")) {
  console.log("üì§ Dumping full schema‚Ä¶");

  const queries = {
    tables: `
      SELECT table_name, column_name, data_type
      FROM information_schema.columns
      WHERE table_schema = 'public'
      ORDER BY table_name, ordinal_position;
    `,
    constraints: `
      SELECT conrelid::regclass AS table_name,
             conname AS constraint_name,
             pg_get_constraintdef(c.oid)
      FROM pg_constraint c
      JOIN pg_namespace n ON n.oid = c.connamespace
      WHERE n.nspname = 'public';
    `,
    indexes: `
      SELECT schemaname, relname AS table_name, indexname, indexdef
      FROM pg_indexes
      WHERE schemaname = 'public';
    `,
    triggers: `
      SELECT event_object_table AS table_name,
             trigger_name,
             event_manipulation AS event,
             action_statement
      FROM information_schema.triggers
      WHERE trigger_schema = 'public';
    `,
    functions: `
      SELECT proname AS function_name,
             pg_get_functiondef(p.oid) AS definition
      FROM pg_proc p
      JOIN pg_namespace n ON n.oid = p.pronamespace
      WHERE n.nspname = 'public';
    `
  };

  let dump = {};
  for (const [key, sql] of Object.entries(queries)) {
    const res = await client.query(sql);
    dump[key] = res.rows;
  }

  fs.mkdirSync("./deploy/exports", { recursive: true });
  fs.writeFileSync("./deploy/exports/schema_dump.json", JSON.stringify(dump, null, 2));

  console.log("‚úÖ Schema dump written to deploy/exports/schema_dump.json");
  await client.end();
  process.exit(0);
}

//
// ========== NEW FEATURE: FUNCTIONS-ONLY MODE ==========
//
if (args.includes("--functions-only")) {
  console.log("‚ö° Running functions-only deployment‚Ä¶");

  const sql = fs.readFileSync("deploy/phase1/functions.sql", "utf8");

  // Split functions by CREATE FUNCTION or CREATE OR REPLACE FUNCTION
  const functions = sql.split(/(?=CREATE\s+(OR\s+REPLACE\s+)?FUNCTION)/gi);

  for (const fn of functions) {
    if (!fn.trim()) continue;
    try {
      await client.query("BEGIN");
      await client.query(fn);
      await client.query("COMMIT");
      console.log(`‚úÖ Function deployed successfully: ${fn.match(/FUNCTION\s+(\w+)/i)?.[1]}`);
    } catch (err) {
      await client.query("ROLLBACK");
      console.error(`‚ùå Function failed: ${fn.match(/FUNCTION\s+(\w+)/i)?.[1]}`);
      console.error(err.message);
      // Optional: send to error server
      try {
        await fetch("http://localhost:4000/save-errors", {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({ sql: fn, error: err.message })
        });
      } catch (e) {
        console.warn("‚ö†Ô∏è Error server not reachable");
      }
    }
  }

  await client.end();
  process.exit(0);
}

// Keep your existing deploy logic for phases here‚Ä¶

üéØ Usage Examples
1. Dump schema
node deploy-enhanced.js --dump


‚û°Ô∏è Writes deploy/exports/schema_dump.json with tables, columns, indexes, constraints, triggers, functions.
Claude can now always check the truth before suggesting changes.

2. Deploy functions only
node deploy-enhanced.js --functions-only


‚û°Ô∏è Reads deploy/phase1/functions.sql and applies each function one-by-one.
‚û°Ô∏è If one fails, it logs and continues instead of rolling back all.
‚û°Ô∏è Errors sent to error server if running.

3. Normal phased deployment (unchanged)
node deploy-enhanced.js --phase=1


‚úÖ With this, Claude can:

Always pull a fresh schema snapshot (no guessing).

Always safely re-deploy functions in a loop without rollback nukes.

Keep using existing phase-based deploys for enums/tables/columns.

üíØ Exactly ‚Äî if the goal is:

Claude should be able to fix loop anything in the database (tables, columns, constraints, triggers, indexes, functions, seed data, schema snapshots) without you needing to step in

‚Ä¶then the deployer has to become a full-service tool, not just a phased migration script.

Right now, with the changes we just added, Claude can:

‚úÖ Run migrations phase-by-phase (--phase=1 etc.)

‚úÖ Deploy functions safely (--functions-only)

‚úÖ Dump schema (--dump)

But he‚Äôs still limited ‚Äî if he needs to patch just constraints, just triggers, just indexes, or seed data, he doesn‚Äôt have modes for that.

üîé What‚Äôs Missing (to let Claude do anything)

Targeted Modes for Each Object Type

--enums-only (idempotent enum deployment)

--tables-only (CREATE/ALTER TABLE blocks)

--columns-only (ALTER TABLE ADD COLUMN, etc.)

--indexes-only

--constraints-only

--triggers-only

--functions-only (we already built this ‚úÖ)

Seed Data Support

A --seed mode that reads /deploy/seed/seed.sql and runs inserts/upserts.

Ensures demo data, default templates, etc. can always be restored.

Rollback / Replay

A --rollback=phaseX to undo a failed deploy.

A --replay=phaseX to re-run just that phase.

Full Schema Export (already added)

‚úÖ --dump writes schema_dump.json.

Claude can diff this against locked schema to know exactly what‚Äôs drifted.

Error Handling Improvements

Each mode should log failures but continue, so Claude sees which specific statement failed.

Right now, only functions do this ‚Äî the other layers still rollback on error.

üéØ The End State (Claude-Proof Deployer)

Claude should be able to run:

node deploy-enhanced.js --dump                   # See full live schema
node deploy-enhanced.js --tables-only            # Fix table definitions
node deploy-enhanced.js --columns-only           # Patch missing columns
node deploy-enhanced.js --constraints-only       # Re-deploy constraints
node deploy-enhanced.js --indexes-only           # Re-deploy indexes
node deploy-enhanced.js --triggers-only          # Re-deploy triggers
node deploy-enhanced.js --functions-only         # Fix functions
node deploy-enhanced.js --seed                   # Load default data
node deploy-enhanced.js --rollback=phase1        # Undo phase 1 changes
node deploy-enhanced.js --replay=phase2          # Retry a failed phase

‚úÖ What We Already Have

Phase-based deploys (--phase=N)

Functions-only (--functions-only)

Schema dump (--dump)

‚ùå What We Still Need to Add

--enums-only

--tables-only

--columns-only

--constraints-only

--indexes-only

--triggers-only

--seed

--rollback / --replay

üëâ Do you want me to extend deploy-enhanced.js with all those modes in one go, so Claude truly has a universal deployer and can run any operation without asking you to step in?

You said:
yes
ChatGPT said:

Perfect ‚úÖ ‚Äî if we want Claude to do anything in his fix loop, then the deployer must cover all object types, seed data, and rollback/replay.

Here‚Äôs the full upgrade plan for deploy-enhanced.js.

üìã Universal Deployer Upgrade
üîé New Capabilities

--enums-only ‚Üí deploy enums.

--tables-only ‚Üí deploy tables.

--columns-only ‚Üí deploy columns.

--constraints-only ‚Üí deploy constraints.

--indexes-only ‚Üí deploy indexes.

--triggers-only ‚Üí deploy triggers.

--functions-only ‚Üí deploy functions (already added ‚úÖ).

--seed ‚Üí load seed data.

--rollback=phaseX ‚Üí undo a phase.

--replay=phaseX ‚Üí re-run a phase.

--dump ‚Üí schema snapshot (already added ‚úÖ).

üõ†Ô∏è Code Additions (extend your deployer)

Below is a template snippet. You‚Äôll plug in the same ‚Äúread file ‚Üí run queries‚Äù pattern you already use for phases.

// Utility: run SQL file safely
async function runSqlFile(path, label) {
  if (!fs.existsSync(path)) {
    console.warn(`‚ö†Ô∏è Skipping ${label}, no file at ${path}`);
    return;
  }
  const sql = fs.readFileSync(path, "utf8");
  const statements = sql.split(/;\s*$/m).filter(s => s.trim());
  for (const stmt of statements) {
    try {
      await client.query("BEGIN");
      await client.query(stmt);
      await client.query("COMMIT");
      console.log(`‚úÖ ${label}: executed`);
    } catch (err) {
      await client.query("ROLLBACK");
      console.error(`‚ùå ${label} failed:`, err.message);
      // send to error server if running
      try {
        await fetch("http://localhost:4000/save-errors", {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({ sql: stmt, error: err.message })
        });
      } catch {}
    }
  }
}

// ===== Enums Only =====
if (args.includes("--enums-only")) {
  await runSqlFile("deploy/phase1/enums.sql", "Enums");
  await client.end(); process.exit(0);
}

// ===== Tables Only =====
if (args.includes("--tables-only")) {
  await runSqlFile("deploy/phase1/tables.sql", "Tables");
  await client.end(); process.exit(0);
}

// ===== Columns Only =====
if (args.includes("--columns-only")) {
  await runSqlFile("deploy/phase1/columns.sql", "Columns");
  await client.end(); process.exit(0);
}

// ===== Constraints Only =====
if (args.includes("--constraints-only")) {
  await runSqlFile("deploy/phase1/constraints.sql", "Constraints");
  await client.end(); process.exit(0);
}

// ===== Indexes Only =====
if (args.includes("--indexes-only")) {
  await runSqlFile("deploy/phase1/indexes.sql", "Indexes");
  await client.end(); process.exit(0);
}

// ===== Triggers Only =====
if (args.includes("--triggers-only")) {
  await runSqlFile("deploy/phase1/triggers.sql", "Triggers");
  await client.end(); process.exit(0);
}

// ===== Functions Only (already added, reuse safe loop) =====

// ===== Seed Data =====
if (args.includes("--seed")) {
  await runSqlFile("deploy/seed/seed.sql", "Seed Data");
  await client.end(); process.exit(0);
}

// ===== Rollback =====
const rollbackMatch = args.find(a => a.startsWith("--rollback="));
if (rollbackMatch) {
  const phase = rollbackMatch.split("=")[1];
  const file = `deploy/${phase}/rollback.sql`;
  await runSqlFile(file, `Rollback ${phase}`);
  await client.end(); process.exit(0);
}

// ===== Replay =====
const replayMatch = args.find(a => a.startsWith("--replay="));
if (replayMatch) {
  const phase = replayMatch.split("=")[1];
  const file = `deploy/${phase}/all.sql`;
  await runSqlFile(file, `Replay ${phase}`);
  await client.end(); process.exit(0);
}

üìÇ Folder Structure to Support This
deploy/
  phase1/
    enums.sql
    tables.sql
    columns.sql
    constraints.sql
    indexes.sql
    triggers.sql
    functions.sql
    rollback.sql
    all.sql          <-- combined (for --replay)
  seed/
    seed.sql
  exports/
    schema_dump.json <-- from --dump

üéØ Result

Claude can now:

See schema truth:

node deploy-enhanced.js --dump


Fix only the part that‚Äôs broken:

node deploy-enhanced.js --columns-only
node deploy-enhanced.js --functions-only
node deploy-enhanced.js --constraints-only


Load seeds:

node deploy-enhanced.js --seed


Rollback or replay phases:

node deploy-enhanced.js --rollback=phase1
node deploy-enhanced.js --replay=phase1


‚úÖ This means Claude can now fully self-heal:

Pull live schema ‚Üí compare to locked schema ‚Üí generate fix ‚Üí deploy just that piece.

No more full rollback loops unless he chooses to.
üëâ Do you want me to generate ready-to-run rollback.sql & all.sql templates for Phase 1 so Claude has a starting point for those files?

You said:
yes
ChatGPT said:

Perfect ‚úÖ ‚Äî let‚Äôs make sure Phase 1 has the rollback and replay scaffolding Claude needs.

üìÇ Folder Layout for Phase 1
deploy/
  phase1/
    enums.sql
    tables.sql
    columns.sql
    constraints.sql
    indexes.sql
    triggers.sql
    functions.sql
    rollback.sql     <-- NEW
    all.sql          <-- NEW

üìã rollback.sql (Phase 1)

This file should undo everything Phase 1 created. Since Postgres doesn‚Äôt support IF EXISTS for every type, we‚Äôll wrap them in DROP ‚Ä¶ IF EXISTS.

-- Phase 1 Rollback
-- Drop functions first (depends on tables)
DROP FUNCTION IF EXISTS validate_onboarding_step(UUID, INT);

-- Drop triggers
DROP TRIGGER IF EXISTS onboarding_progress_trigger ON onboarding_progress;

-- Drop indexes
DROP INDEX IF EXISTS idx_rate_cards_company;
DROP INDEX IF EXISTS idx_onboarding_progress_company;

-- Drop constraints (safe to ignore if they don‚Äôt exist)
ALTER TABLE IF EXISTS onboarding_progress DROP CONSTRAINT IF EXISTS onboarding_progress_company_id_fkey;
ALTER TABLE IF EXISTS rate_cards DROP CONSTRAINT IF EXISTS rate_cards_company_id_fkey;

-- Drop tables (reverse order of dependencies)
DROP TABLE IF EXISTS onboarding_progress CASCADE;
DROP TABLE IF EXISTS document_templates CASCADE;
DROP TABLE IF EXISTS payment_settings CASCADE;
DROP TABLE IF EXISTS rate_cards CASCADE;
DROP TABLE IF EXISTS company_settings CASCADE;

-- Drop enums
DROP TYPE IF EXISTS document_type_enum CASCADE;
DROP TYPE IF EXISTS unit_type_enum CASCADE;


‚ö†Ô∏è Rollbacks are destructive. Claude should only run this if he intends to wipe Phase 1 and replay.

üìã all.sql (Phase 1)

This file simply chains all the other scripts in the right order. Claude can replay everything in one go.

\i deploy/phase1/enums.sql
\i deploy/phase1/tables.sql
\i deploy/phase1/columns.sql
\i deploy/phase1/constraints.sql
\i deploy/phase1/indexes.sql
\i deploy/phase1/triggers.sql
\i deploy/phase1/functions.sql


Note: \i is the psql include command. If Claude uses Node‚Äôs pg client, the deployer can instead concatenate these files programmatically. Either works as long as you‚Äôre consistent.

üéØ Workflow Example

Claude detects a mismatch.

He runs rollback:

node deploy-enhanced.js --rollback=phase1


‚Üí Drops Phase 1 objects safely.

He runs replay:

node deploy-enhanced.js --replay=phase1


‚Üí Re-applies all Phase 1 files (all.sql).

‚úÖ Result

Rollback = nuke & reset

Replay = clean reapply

Claude can now self-heal Phase 1 if migrations drift, without you intervening.

üíØ Got it. No more piecemeal. Let‚Äôs make one big universal package so Claude has everything he needs to run fix loops end-to-end ‚Äî schema introspection, targeted deploys, rollback, replay, seed data, error logging.

üìã Universal Claude-Proof Deployer Framework
üîé Capabilities Claude Needs

Introspection (truth source)

--dump ‚Üí exports full schema (tables, columns, constraints, indexes, triggers, functions).

Targeted Deploy Modes

--enums-only

--tables-only

--columns-only

--constraints-only

--indexes-only

--triggers-only

--functions-only

--seed

Recovery Modes

--rollback=phaseX

--replay=phaseX

Error Handling

Each object runs in its own BEGIN/COMMIT so one failure doesn‚Äôt kill the batch.

Failed statements are logged to error server.

üìÇ Folder Structure (Standardized)
deploy/
  phase1/
    enums.sql
    tables.sql
    columns.sql
    constraints.sql
    indexes.sql
    triggers.sql
    functions.sql
    rollback.sql
    all.sql
  phase2/
    enums.sql
    tables.sql
    columns.sql
    constraints.sql
    indexes.sql
    triggers.sql
    functions.sql
    rollback.sql
    all.sql
  seed/
    seed.sql
  exports/
    schema_dump.json

üõ†Ô∏è Universal deploy-enhanced.js
#!/usr/bin/env node
import fs from "fs";
import pg from "pg";
import dotenv from "dotenv";
dotenv.config();

const { Client } = pg;
const args = process.argv.slice(2);
const connString = process.env.DATABASE_URL || process.env.DB_URL;

if (!connString) {
  console.error("‚ùå Missing DATABASE_URL or DB_URL in .env");
  process.exit(1);
}

const client = new Client({ connectionString: connString, ssl: { rejectUnauthorized: false } });
await client.connect();

// Helper: run SQL file safely
async function runSqlFile(path, label) {
  if (!fs.existsSync(path)) {
    console.warn(`‚ö†Ô∏è Skipping ${label}, no file at ${path}`);
    return;
  }
  const sql = fs.readFileSync(path, "utf8");
  const statements = sql.split(/;\s*$/m).filter(s => s.trim());
  for (const stmt of statements) {
    try {
      await client.query("BEGIN");
      await client.query(stmt);
      await client.query("COMMIT");
      console.log(`‚úÖ ${label} executed`);
    } catch (err) {
      await client.query("ROLLBACK");
      console.error(`‚ùå ${label} failed:`, err.message);
      try {
        await fetch("http://localhost:4000/save-errors", {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({ sql: stmt, error: err.message })
        });
      } catch {}
    }
  }
}

// ====== SCHEMA DUMP ======
if (args.includes("--dump")) {
  console.log("üì§ Dumping full schema‚Ä¶");
  const queries = {
    tables: `
      SELECT table_name, column_name, data_type
      FROM information_schema.columns
      WHERE table_schema = 'public'
      ORDER BY table_name, ordinal_position;
    `,
    constraints: `
      SELECT conrelid::regclass AS table_name,
             conname AS constraint_name,
             pg_get_constraintdef(c.oid)
      FROM pg_constraint c
      JOIN pg_namespace n ON n.oid = c.connamespace
      WHERE n.nspname = 'public';
    `,
    indexes: `
      SELECT schemaname, relname AS table_name, indexname, indexdef
      FROM pg_indexes
      WHERE schemaname = 'public';
    `,
    triggers: `
      SELECT event_object_table AS table_name,
             trigger_name,
             event_manipulation AS event,
             action_statement
      FROM information_schema.triggers
      WHERE trigger_schema = 'public';
    `,
    functions: `
      SELECT proname AS function_name,
             pg_get_functiondef(p.oid) AS definition
      FROM pg_proc p
      JOIN pg_namespace n ON n.oid = p.pronamespace
      WHERE n.nspname = 'public';
    `
  };

  let dump = {};
  for (const [key, sql] of Object.entries(queries)) {
    const res = await client.query(sql);
    dump[key] = res.rows;
  }

  fs.mkdirSync("./deploy/exports", { recursive: true });
  fs.writeFileSync("./deploy/exports/schema_dump.json", JSON.stringify(dump, null, 2));
  console.log("‚úÖ Schema dump written to deploy/exports/schema_dump.json");
  await client.end();
  process.exit(0);
}

// ====== TARGETED MODES ======
const modes = [
  { arg: "--enums-only", path: "deploy/phase1/enums.sql", label: "Enums" },
  { arg: "--tables-only", path: "deploy/phase1/tables.sql", label: "Tables" },
  { arg: "--columns-only", path: "deploy/phase1/columns.sql", label: "Columns" },
  { arg: "--constraints-only", path: "deploy/phase1/constraints.sql", label: "Constraints" },
  { arg: "--indexes-only", path: "deploy/phase1/indexes.sql", label: "Indexes" },
  { arg: "--triggers-only", path: "deploy/phase1/triggers.sql", label: "Triggers" },
  { arg: "--functions-only", path: "deploy/phase1/functions.sql", label: "Functions" },
  { arg: "--seed", path: "deploy/seed/seed.sql", label: "Seed Data" }
];

for (const mode of modes) {
  if (args.includes(mode.arg)) {
    await runSqlFile(mode.path, mode.label);
    await client.end();
    process.exit(0);
  }
}

// ====== ROLLBACK ======
const rollbackMatch = args.find(a => a.startsWith("--rollback="));
if (rollbackMatch) {
  const phase = rollbackMatch.split("=")[1];
  await runSqlFile(`deploy/${phase}/rollback.sql`, `Rollback ${phase}`);
  await client.end();
  process.exit(0);
}

// ====== REPLAY ======
const replayMatch = args.find(a => a.startsWith("--replay="));
if (replayMatch) {
  const phase = replayMatch.split("=")[1];
  await runSqlFile(`deploy/${phase}/all.sql`, `Replay ${phase}`);
  await client.end();
  process.exit(0);
}

// If no args, show help
console.log(`
Usage:
  node deploy-enhanced.js --dump
  node deploy-enhanced.js --enums-only | --tables-only | --columns-only | --constraints-only | --indexes-only | --triggers-only | --functions-only
  node deploy-enhanced.js --seed
  node deploy-enhanced.js --rollback=phase1
  node deploy-enhanced.js --replay=phase1
`);
await client.end();

üìã rollback.sql Templates
Phase 1
-- Phase 1 Rollback
DROP FUNCTION IF EXISTS validate_onboarding_step(UUID, INT);

DROP TRIGGER IF EXISTS onboarding_progress_trigger ON onboarding_progress;

DROP INDEX IF EXISTS idx_rate_cards_company;
DROP INDEX IF EXISTS idx_onboarding_progress_company;

ALTER TABLE IF EXISTS onboarding_progress DROP CONSTRAINT IF EXISTS onboarding_progress_company_id_fkey;
ALTER TABLE IF EXISTS rate_cards DROP CONSTRAINT IF EXISTS rate_cards_company_id_fkey;

DROP TABLE IF EXISTS onboarding_progress CASCADE;
DROP TABLE IF EXISTS document_templates CASCADE;
DROP TABLE IF EXISTS payment_settings CASCADE;
DROP TABLE IF EXISTS rate_cards CASCADE;
DROP TABLE IF EXISTS company_settings CASCADE;

DROP TYPE IF EXISTS document_type_enum CASCADE;
DROP TYPE IF EXISTS unit_type_enum CASCADE;

Phase 2
-- Phase 2 Rollback
-- Drop functions
DROP FUNCTION IF EXISTS some_phase2_function();

-- Drop triggers
DROP TRIGGER IF EXISTS some_phase2_trigger ON some_phase2_table;

-- Drop indexes
DROP INDEX IF EXISTS idx_phase2_example;

-- Drop constraints
ALTER TABLE IF EXISTS some_phase2_table DROP CONSTRAINT IF EXISTS fk_phase2_example;

-- Drop tables
DROP TABLE IF EXISTS some_phase2_table CASCADE;
DROP TABLE IF EXISTS another_phase2_table CASCADE;

-- Drop enums
DROP TYPE IF EXISTS phase2_status_enum CASCADE;


(fill in actual names from Phase 2 scripts once confirmed)

üìã all.sql Templates
Phase 1
\i deploy/phase1/enums.sql
\i deploy/phase1/tables.sql
\i deploy/phase1/columns.sql
\i deploy/phase1/constraints.sql
\i deploy/phase1/indexes.sql
\i deploy/phase1/triggers.sql
\i deploy/phase1/functions.sql

Phase 2
\i deploy/phase2/enums.sql
\i deploy/phase2/tables.sql
\i deploy/phase2/columns.sql
\i deploy/phase2/constraints.sql
\i deploy/phase2/indexes.sql
\i deploy/phase2/triggers.sql
\i deploy/phase2/functions.sql

üéØ End Result

Claude can dump schema truth at any time.

Claude can deploy any piece (enums, tables, columns, constraints, indexes, triggers, functions, seed data).

Claude can rollback/replay entire phases to reset drift.

All failures are logged individually, not nuking entire transactions.

This means Claude can now run fix loops fully autonomously.